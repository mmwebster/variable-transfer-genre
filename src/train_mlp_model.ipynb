{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('genre_classification_289a/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from model import STN, MLP\n",
    "import torch.optim as optim\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import get_data_loaders, FramedFeatureDataset, FeatureDataset, DatasetSettings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP target always genre\n",
    "agfs = [] #'subgenre', 'mfcc'\n",
    "genre = True #False if not genre STN\n",
    "    \n",
    "#dataset\n",
    "dataset_name = 'fma_medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num genres:  16\n",
      "Rock                   6911\n",
      "Electronic             6110\n",
      "Experimental           2207\n",
      "Hip-Hop                2109\n",
      "Folk                   1477\n",
      "Instrumental           1280\n",
      "Pop                    1129\n",
      "International          1004\n",
      "Classical               598\n",
      "Old-Time / Historic     510\n",
      "Jazz                    380\n",
      "Country                 178\n",
      "Soul-RnB                154\n",
      "Spoken                  118\n",
      "Blues                    72\n",
      "Easy Listening           21\n",
      "Name: genre_top, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "settings = DatasetSettings(dataset_name, 'fma_metadata')\n",
    "dataset = FramedFeatureDataset(settings,  agfs=agfs, genre=genre)\n",
    "print(\"Num genres: \", settings.num_genres)\n",
    "print(settings.genre_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stn_path(dataset, target):\n",
    "    return '../models/DCNN_{}_{}'.format(dataset, target)\n",
    "\n",
    "# load STNs\n",
    "# @NOTE: order them by ['subgenres', 'mfcc', 'genre'] in order for analysis plots to automatically work\n",
    "targets = ['subgenres', 'mfcc', 'genre']\n",
    "stns = [torch.load(get_stn_path(dataset_name, target)).to(device) for target in targets]\n",
    "stn_layer_dims = [None, 16, 32, 64, 64, 128, 256, 256]\n",
    "\n",
    "#which layer to extract features from\n",
    "layers = [6,6,7]\n",
    "layers_str = \"\".join([str(l) for l in layers])\n",
    "\n",
    "# setup MLP on GPU\n",
    "mlp_input_size = np.array([stn_layer_dims[layer] for layer in layers]).sum()\n",
    "mlp_output_size = settings.num_genres\n",
    "mlp = MLP(mlp_input_size, mlp_output_size)\n",
    "mlp.to(device)\n",
    "mlp = nn.DataParallel(mlp)\n",
    "\n",
    "## Training Parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "valid_split = 0.2\n",
    "\n",
    "trainloader, validloader = get_data_loaders(dataset, batch_size, valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_val_loss():\n",
    "    with torch.no_grad():\n",
    "        for stn in stns:\n",
    "            stn.eval()\n",
    "        mlp.eval()\n",
    "        \n",
    "        # start at a random batch and only eval that one batch\n",
    "        init_idx = np.random.randint(len(validloader))\n",
    "\n",
    "        val_loss = 0\n",
    "        for j, data in enumerate(validloader, init_idx):\n",
    "            inputs, labels = data[0].to(device), data[1]['genre'].to(device)\n",
    "\n",
    "            out_intermediate = [stn.module.forward_intermediate(inputs, layers[i]) for i,stn in enumerate(stns)]\n",
    "            input_mlp = torch.cat(out_intermediate, dim=1)\n",
    "\n",
    "            out = mlp(input_mlp)\n",
    "            val_loss = F.cross_entropy(out, labels)\n",
    "            break\n",
    "        \n",
    "        return val_loss\n",
    "            \n",
    "\n",
    "# @param fast Compute validation F1 score for only one batch\n",
    "def validate(fast = False):\n",
    "    with torch.no_grad():\n",
    "        for stn in stns:\n",
    "            stn.eval()\n",
    "        mlp.eval()\n",
    "                \n",
    "        all_pred = []\n",
    "        all_true = []\n",
    "        \n",
    "        if fast:\n",
    "            # start at a random batch if fast == True\n",
    "            init_idx = np.random.randint(len(validloader))\n",
    "        else:\n",
    "            init_idx = 0\n",
    "        \n",
    "        for i, data in enumerate(validloader, init_idx):\n",
    "            inputs, labels = data[0].to(device), data[1]['genre'].to(device)\n",
    "            \n",
    "            out_intermediate = [stn.module.forward_intermediate(inputs, layers[i]) for i,stn in enumerate(stns)]\n",
    "            input_mlp = torch.cat(out_intermediate, dim=1)\n",
    "            \n",
    "            out = mlp(input_mlp)\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            \n",
    "            all_pred.append(out.argmax(dim=1))\n",
    "            all_true.append(labels)\n",
    "            \n",
    "            # only compute val accuracy for one random batch if fast == True\n",
    "            if fast:\n",
    "                break\n",
    "            \n",
    "        all_pred = torch.cat(all_pred)\n",
    "        all_true = torch.cat(all_true)\n",
    "        \n",
    "        curr_f1 = f1_score(all_true.cpu(), all_pred.cpu(), average='micro')\n",
    "        return curr_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.63 Âµs\n",
      "Starting to train at:  2020-05-09 02:02:58.824310  (time is +7 w.r.t. our timezone)\n",
      "Starting epoch 1\n",
      "[1,    30] loss: 1.601, val-loss: 1.011 @ 2020-05-09 02:03:02.980136\n",
      "[1,    60] loss: 0.853, val-loss: 0.652 @ 2020-05-09 02:03:05.917239\n",
      "[1,    90] loss: 0.619, val-loss: 0.455 @ 2020-05-09 02:03:08.871687\n",
      "[1,   120] loss: 0.534, val-loss: 0.364 @ 2020-05-09 02:03:11.807746\n",
      "[1,   150] loss: 0.449, val-loss: 0.364 @ 2020-05-09 02:03:29.114115\n",
      "[1,   180] loss: 0.362, val-loss: 0.290 @ 2020-05-09 02:03:56.647104\n",
      "[1,   210] loss: 0.303, val-loss: 0.238 @ 2020-05-09 02:04:21.103603\n",
      "[1,   240] loss: 0.347, val-loss: 0.197 @ 2020-05-09 02:04:43.741988\n",
      "[1,   270] loss: 0.301, val-loss: 0.135 @ 2020-05-09 02:05:03.768120\n",
      "[1,   300] loss: 0.314, val-loss: 0.173 @ 2020-05-09 02:05:25.265068\n",
      "[1,   330] loss: 0.312, val-loss: 0.290 @ 2020-05-09 02:05:47.551788\n",
      "[1,   360] loss: 0.268, val-loss: 0.106 @ 2020-05-09 02:06:05.720385\n",
      "[1,   390] loss: 0.218, val-loss: 0.117 @ 2020-05-09 02:06:27.888363\n",
      "[1,   420] loss: 0.243, val-loss: 0.131 @ 2020-05-09 02:06:46.768049\n",
      "[1,   450] loss: 0.272, val-loss: 0.240 @ 2020-05-09 02:07:09.527791\n",
      "[1,   480] loss: 0.285, val-loss: 0.147 @ 2020-05-09 02:07:28.311269\n",
      "[1,   510] loss: 0.190, val-loss: 0.109 @ 2020-05-09 02:07:48.289703\n",
      "[1,   540] loss: 0.227, val-loss: 0.108 @ 2020-05-09 02:08:09.835582\n",
      "[1,   570] loss: 0.211, val-loss: 0.079 @ 2020-05-09 02:08:27.884516\n",
      "[1,   600] loss: 0.242, val-loss: 0.188 @ 2020-05-09 02:08:48.701184\n",
      "[1,   630] loss: 0.220, val-loss: 0.122 @ 2020-05-09 02:09:09.784349\n",
      "[1,   660] loss: 0.201, val-loss: 0.118 @ 2020-05-09 02:09:31.190816\n",
      "[1,   690] loss: 0.205, val-loss: 0.222 @ 2020-05-09 02:09:51.596760\n",
      "[1,   720] loss: 0.195, val-loss: 0.144 @ 2020-05-09 02:10:13.949800\n",
      "[1,   750] loss: 0.196, val-loss: 0.144 @ 2020-05-09 02:10:34.952333\n",
      "[1,   780] loss: 0.196, val-loss: 0.075 @ 2020-05-09 02:10:57.760157\n",
      "[1,   810] loss: 0.159, val-loss: 0.133 @ 2020-05-09 02:11:16.503093\n",
      "[1,   840] loss: 0.161, val-loss: 0.136 @ 2020-05-09 02:11:40.561988\n",
      "[1,   870] loss: 0.206, val-loss: 0.183 @ 2020-05-09 02:12:00.373523\n",
      "[1,   900] loss: 0.201, val-loss: 0.193 @ 2020-05-09 02:12:21.869258\n",
      "[1,   930] loss: 0.184, val-loss: 0.234 @ 2020-05-09 02:12:42.230624\n",
      "[1,   960] loss: 0.189, val-loss: 0.177 @ 2020-05-09 02:13:03.778226\n",
      "[1,   990] loss: 0.163, val-loss: 0.144 @ 2020-05-09 02:13:23.956080\n",
      "[1,  1020] loss: 0.185, val-loss: 0.147 @ 2020-05-09 02:13:48.622155\n",
      "[1,  1050] loss: 0.200, val-loss: 0.155 @ 2020-05-09 02:14:09.809870\n",
      "[1,  1080] loss: 0.155, val-loss: 0.133 @ 2020-05-09 02:14:33.096393\n",
      "[1,  1110] loss: 0.178, val-loss: 0.095 @ 2020-05-09 02:14:53.816248\n",
      "[1,  1140] loss: 0.215, val-loss: 0.142 @ 2020-05-09 02:15:16.270438\n",
      "[1,  1170] loss: 0.169, val-loss: 0.155 @ 2020-05-09 02:15:37.850682\n",
      "[1,  1200] loss: 0.158, val-loss: 0.088 @ 2020-05-09 02:16:03.130942\n",
      "[1,  1230] loss: 0.177, val-loss: 0.080 @ 2020-05-09 02:16:24.282318\n",
      "[1,  1260] loss: 0.223, val-loss: 0.162 @ 2020-05-09 02:16:49.600360\n",
      "[1,  1290] loss: 0.197, val-loss: 0.092 @ 2020-05-09 02:17:12.813940\n",
      "[1,  1320] loss: 0.207, val-loss: 0.183 @ 2020-05-09 02:17:36.264403\n",
      "[1,  1350] loss: 0.159, val-loss: 0.190 @ 2020-05-09 02:18:00.620550\n",
      "[1,  1380] loss: 0.169, val-loss: 0.133 @ 2020-05-09 02:18:24.622319\n",
      "[1,  1410] loss: 0.171, val-loss: 0.206 @ 2020-05-09 02:18:48.646299\n",
      "[1,  1440] loss: 0.192, val-loss: 0.148 @ 2020-05-09 02:19:09.060246\n",
      "[1,  1470] loss: 0.129, val-loss: 0.264 @ 2020-05-09 02:19:34.769180\n",
      "[1,  1500] loss: 0.171, val-loss: 0.159 @ 2020-05-09 02:19:58.629518\n",
      "[1,  1530] loss: 0.188, val-loss: 0.110 @ 2020-05-09 02:20:23.271869\n",
      "[1,  1560] loss: 0.179, val-loss: 0.037 @ 2020-05-09 02:20:46.905097\n",
      "[1,  1590] loss: 0.167, val-loss: 0.113 @ 2020-05-09 02:21:13.934328\n",
      "[1,  1620] loss: 0.169, val-loss: 0.086 @ 2020-05-09 02:21:37.190901\n",
      "[1,  1650] loss: 0.161, val-loss: 0.128 @ 2020-05-09 02:22:02.127975\n",
      "[1,  1680] loss: 0.143, val-loss: 0.087 @ 2020-05-09 02:22:26.378322\n",
      "[1,  1710] loss: 0.121, val-loss: 0.065 @ 2020-05-09 02:22:53.166795\n",
      "[1,  1740] loss: 0.178, val-loss: 0.042 @ 2020-05-09 02:23:16.791468\n",
      "[1,  1770] loss: 0.184, val-loss: 0.118 @ 2020-05-09 02:23:41.866864\n",
      "[1,  1800] loss: 0.183, val-loss: 0.094 @ 2020-05-09 02:24:06.459410\n",
      "[1,  1830] loss: 0.168, val-loss: 0.100 @ 2020-05-09 02:24:32.118467\n",
      "[1,  1860] loss: 0.172, val-loss: 0.136 @ 2020-05-09 02:24:57.693560\n",
      "[1,  1890] loss: 0.187, val-loss: 0.051 @ 2020-05-09 02:25:22.326334\n",
      "[1,  1920] loss: 0.183, val-loss: 0.101 @ 2020-05-09 02:25:46.368457\n",
      "[1,  1950] loss: 0.152, val-loss: 0.046 @ 2020-05-09 02:26:13.182304\n",
      "[1,  1980] loss: 0.157, val-loss: 0.053 @ 2020-05-09 02:26:39.137102\n",
      "[1,  2010] loss: 0.149, val-loss: 0.088 @ 2020-05-09 02:27:07.173312\n",
      "[1,  2040] loss: 0.133, val-loss: 0.095 @ 2020-05-09 02:27:34.357555\n",
      "[1,  2070] loss: 0.157, val-loss: 0.090 @ 2020-05-09 02:28:02.908293\n",
      "[1,  2100] loss: 0.170, val-loss: 0.088 @ 2020-05-09 02:28:29.620187\n",
      "Starting epoch 2\n",
      "[2,    30] loss: 0.118, val-loss: 0.078 @ 2020-05-09 02:29:18.677673\n",
      "[2,    60] loss: 0.103, val-loss: 0.079 @ 2020-05-09 02:29:45.258040\n",
      "[2,    90] loss: 0.130, val-loss: 0.027 @ 2020-05-09 02:30:10.534174\n",
      "[2,   120] loss: 0.144, val-loss: 0.102 @ 2020-05-09 02:30:33.861482\n",
      "[2,   150] loss: 0.149, val-loss: 0.037 @ 2020-05-09 02:31:00.867907\n",
      "[2,   180] loss: 0.136, val-loss: 0.086 @ 2020-05-09 02:31:26.032995\n",
      "[2,   210] loss: 0.093, val-loss: 0.226 @ 2020-05-09 02:31:54.165286\n",
      "[2,   240] loss: 0.099, val-loss: 0.147 @ 2020-05-09 02:32:18.253998\n",
      "[2,   270] loss: 0.128, val-loss: 0.040 @ 2020-05-09 02:32:45.769442\n",
      "[2,   300] loss: 0.156, val-loss: 0.066 @ 2020-05-09 02:33:13.880267\n",
      "[2,   330] loss: 0.088, val-loss: 0.042 @ 2020-05-09 02:33:41.019093\n",
      "[2,   360] loss: 0.103, val-loss: 0.081 @ 2020-05-09 02:34:09.251815\n",
      "[2,   390] loss: 0.132, val-loss: 0.041 @ 2020-05-09 02:34:35.277909\n",
      "[2,   420] loss: 0.152, val-loss: 0.100 @ 2020-05-09 02:35:04.612206\n",
      "[2,   450] loss: 0.123, val-loss: 0.103 @ 2020-05-09 02:35:30.960277\n",
      "[2,   480] loss: 0.115, val-loss: 0.090 @ 2020-05-09 02:36:02.972094\n",
      "[2,   510] loss: 0.100, val-loss: 0.090 @ 2020-05-09 02:36:30.651880\n",
      "[2,   540] loss: 0.131, val-loss: 0.088 @ 2020-05-09 02:36:59.685485\n",
      "[2,   570] loss: 0.117, val-loss: 0.030 @ 2020-05-09 02:37:27.511024\n",
      "[2,   600] loss: 0.113, val-loss: 0.132 @ 2020-05-09 02:37:58.417749\n",
      "[2,   630] loss: 0.133, val-loss: 0.124 @ 2020-05-09 02:38:24.546977\n",
      "[2,   660] loss: 0.120, val-loss: 0.087 @ 2020-05-09 02:38:56.376082\n",
      "[2,   690] loss: 0.148, val-loss: 0.090 @ 2020-05-09 02:39:24.900708\n",
      "[2,   720] loss: 0.141, val-loss: 0.038 @ 2020-05-09 02:39:54.472291\n",
      "[2,   750] loss: 0.133, val-loss: 0.098 @ 2020-05-09 02:40:23.620269\n",
      "[2,   780] loss: 0.122, val-loss: 0.022 @ 2020-05-09 02:40:54.570230\n",
      "[2,   810] loss: 0.132, val-loss: 0.026 @ 2020-05-09 02:41:23.672576\n",
      "[2,   840] loss: 0.143, val-loss: 0.005 @ 2020-05-09 02:41:53.956984\n",
      "[2,   870] loss: 0.137, val-loss: 0.007 @ 2020-05-09 02:42:23.588302\n",
      "[2,   900] loss: 0.160, val-loss: 0.052 @ 2020-05-09 02:42:56.014350\n",
      "[2,   930] loss: 0.155, val-loss: 0.064 @ 2020-05-09 02:43:22.319543\n",
      "[2,   960] loss: 0.154, val-loss: 0.016 @ 2020-05-09 02:43:55.232157\n",
      "[2,   990] loss: 0.126, val-loss: 0.034 @ 2020-05-09 02:44:23.284103\n",
      "[2,  1020] loss: 0.128, val-loss: 0.017 @ 2020-05-09 02:44:53.673676\n",
      "[2,  1050] loss: 0.120, val-loss: 0.051 @ 2020-05-09 02:45:24.484468\n",
      "[2,  1080] loss: 0.123, val-loss: 0.063 @ 2020-05-09 02:45:57.751366\n",
      "[2,  1110] loss: 0.106, val-loss: 0.046 @ 2020-05-09 02:46:28.097019\n",
      "[2,  1140] loss: 0.152, val-loss: 0.193 @ 2020-05-09 02:46:59.863000\n",
      "[2,  1170] loss: 0.163, val-loss: 0.130 @ 2020-05-09 02:47:29.244246\n",
      "[2,  1200] loss: 0.147, val-loss: 0.053 @ 2020-05-09 02:48:02.152708\n",
      "[2,  1230] loss: 0.138, val-loss: 0.189 @ 2020-05-09 02:48:33.372875\n",
      "[2,  1260] loss: 0.192, val-loss: 0.196 @ 2020-05-09 02:49:05.240845\n",
      "[2,  1290] loss: 0.144, val-loss: 0.039 @ 2020-05-09 02:49:39.403092\n",
      "[2,  1320] loss: 0.134, val-loss: 0.084 @ 2020-05-09 02:50:11.621941\n",
      "[2,  1350] loss: 0.134, val-loss: 0.179 @ 2020-05-09 02:50:43.645786\n",
      "[2,  1380] loss: 0.112, val-loss: 0.137 @ 2020-05-09 02:51:16.795610\n",
      "[2,  1410] loss: 0.122, val-loss: 0.069 @ 2020-05-09 02:51:47.304806\n",
      "[2,  1440] loss: 0.147, val-loss: 0.040 @ 2020-05-09 02:52:22.649130\n",
      "[2,  1470] loss: 0.141, val-loss: 0.048 @ 2020-05-09 02:52:53.773542\n",
      "[2,  1500] loss: 0.169, val-loss: 0.085 @ 2020-05-09 02:53:28.334005\n",
      "[2,  1530] loss: 0.130, val-loss: 0.106 @ 2020-05-09 02:53:58.949030\n",
      "[2,  1560] loss: 0.109, val-loss: 0.076 @ 2020-05-09 02:54:35.213598\n",
      "[2,  1590] loss: 0.133, val-loss: 0.115 @ 2020-05-09 02:55:07.762866\n",
      "[2,  1620] loss: 0.119, val-loss: 0.122 @ 2020-05-09 02:55:39.960323\n",
      "[2,  1650] loss: 0.141, val-loss: 0.209 @ 2020-05-09 02:56:13.502648\n",
      "[2,  1680] loss: 0.116, val-loss: 0.044 @ 2020-05-09 02:56:45.869962\n",
      "[2,  1710] loss: 0.145, val-loss: 0.061 @ 2020-05-09 02:57:20.908789\n",
      "[2,  1740] loss: 0.151, val-loss: 0.079 @ 2020-05-09 02:57:53.976623\n",
      "[2,  1770] loss: 0.155, val-loss: 0.136 @ 2020-05-09 02:58:27.910544\n",
      "[2,  1800] loss: 0.132, val-loss: 0.161 @ 2020-05-09 02:59:02.121717\n",
      "[2,  1830] loss: 0.106, val-loss: 0.027 @ 2020-05-09 02:59:34.995342\n",
      "[2,  1860] loss: 0.112, val-loss: 0.132 @ 2020-05-09 03:00:10.051960\n",
      "[2,  1890] loss: 0.133, val-loss: 0.116 @ 2020-05-09 03:00:42.321715\n",
      "[2,  1920] loss: 0.106, val-loss: 0.040 @ 2020-05-09 03:01:21.282796\n",
      "[2,  1950] loss: 0.147, val-loss: 0.154 @ 2020-05-09 03:01:52.433524\n",
      "[2,  1980] loss: 0.153, val-loss: 0.120 @ 2020-05-09 03:02:30.824460\n",
      "[2,  2010] loss: 0.130, val-loss: 0.066 @ 2020-05-09 03:03:02.793513\n",
      "[2,  2040] loss: 0.154, val-loss: 0.011 @ 2020-05-09 03:03:39.538701\n",
      "[2,  2070] loss: 0.150, val-loss: 0.069 @ 2020-05-09 03:04:11.541941\n",
      "[2,  2100] loss: 0.149, val-loss: 0.061 @ 2020-05-09 03:04:47.684606\n",
      "Starting epoch 3\n",
      "[3,    30] loss: 0.105, val-loss: 0.033 @ 2020-05-09 03:05:49.950498\n",
      "[3,    60] loss: 0.094, val-loss: 0.041 @ 2020-05-09 03:06:23.719999\n",
      "[3,    90] loss: 0.083, val-loss: 0.110 @ 2020-05-09 03:07:00.653532\n",
      "[3,   120] loss: 0.098, val-loss: 0.153 @ 2020-05-09 03:07:38.441532\n",
      "[3,   150] loss: 0.063, val-loss: 0.064 @ 2020-05-09 03:08:14.989417\n",
      "[3,   180] loss: 0.077, val-loss: 0.168 @ 2020-05-09 03:08:50.060176\n",
      "[3,   210] loss: 0.085, val-loss: 0.045 @ 2020-05-09 03:09:26.897137\n",
      "[3,   240] loss: 0.088, val-loss: 0.046 @ 2020-05-09 03:10:03.725990\n",
      "[3,   270] loss: 0.124, val-loss: 0.106 @ 2020-05-09 03:10:38.375788\n",
      "[3,   300] loss: 0.100, val-loss: 0.126 @ 2020-05-09 03:11:22.227574\n",
      "[3,   330] loss: 0.091, val-loss: 0.034 @ 2020-05-09 03:11:51.007520\n",
      "[3,   360] loss: 0.093, val-loss: 0.046 @ 2020-05-09 03:12:29.726084\n",
      "[3,   390] loss: 0.153, val-loss: 0.135 @ 2020-05-09 03:13:05.643419\n",
      "[3,   420] loss: 0.074, val-loss: 0.209 @ 2020-05-09 03:13:50.803304\n",
      "[3,   450] loss: 0.119, val-loss: 0.131 @ 2020-05-09 03:14:21.844758\n",
      "[3,   480] loss: 0.111, val-loss: 0.058 @ 2020-05-09 03:15:02.336593\n",
      "[3,   510] loss: 0.105, val-loss: 0.025 @ 2020-05-09 03:15:36.208698\n",
      "[3,   540] loss: 0.106, val-loss: 0.025 @ 2020-05-09 03:16:15.561671\n",
      "[3,   570] loss: 0.080, val-loss: 0.020 @ 2020-05-09 03:16:49.841542\n",
      "[3,   600] loss: 0.114, val-loss: 0.031 @ 2020-05-09 03:17:28.476398\n",
      "[3,   630] loss: 0.091, val-loss: 0.055 @ 2020-05-09 03:18:03.747101\n",
      "[3,   660] loss: 0.097, val-loss: 0.161 @ 2020-05-09 03:18:44.865192\n",
      "[3,   690] loss: 0.128, val-loss: 0.100 @ 2020-05-09 03:19:19.456117\n",
      "[3,   720] loss: 0.110, val-loss: 0.044 @ 2020-05-09 03:20:04.399062\n",
      "[3,   750] loss: 0.103, val-loss: 0.066 @ 2020-05-09 03:20:38.048838\n",
      "[3,   780] loss: 0.103, val-loss: 0.061 @ 2020-05-09 03:21:17.875803\n",
      "[3,   810] loss: 0.108, val-loss: 0.107 @ 2020-05-09 03:21:55.321954\n",
      "[3,   840] loss: 0.133, val-loss: 0.145 @ 2020-05-09 03:22:32.490513\n",
      "[3,   870] loss: 0.109, val-loss: 0.111 @ 2020-05-09 03:23:13.010987\n",
      "[3,   900] loss: 0.119, val-loss: 0.228 @ 2020-05-09 03:23:51.635953\n",
      "[3,   930] loss: 0.082, val-loss: 0.157 @ 2020-05-09 03:24:29.578321\n",
      "[3,   960] loss: 0.086, val-loss: 0.130 @ 2020-05-09 03:25:08.692797\n",
      "[3,   990] loss: 0.121, val-loss: 0.053 @ 2020-05-09 03:25:48.279918\n",
      "[3,  1020] loss: 0.110, val-loss: 0.053 @ 2020-05-09 03:26:34.684806\n",
      "[3,  1050] loss: 0.084, val-loss: 0.083 @ 2020-05-09 03:27:06.041980\n",
      "[3,  1080] loss: 0.113, val-loss: 0.052 @ 2020-05-09 03:27:47.323696\n",
      "[3,  1110] loss: 0.143, val-loss: 0.058 @ 2020-05-09 03:28:23.325698\n",
      "[3,  1140] loss: 0.100, val-loss: 0.058 @ 2020-05-09 03:29:04.665806\n",
      "[3,  1170] loss: 0.076, val-loss: 0.053 @ 2020-05-09 03:29:40.796468\n",
      "[3,  1200] loss: 0.082, val-loss: 0.234 @ 2020-05-09 03:30:22.186491\n",
      "[3,  1230] loss: 0.119, val-loss: 0.219 @ 2020-05-09 03:31:00.669853\n",
      "[3,  1260] loss: 0.102, val-loss: 0.197 @ 2020-05-09 03:31:40.785334\n",
      "[3,  1290] loss: 0.106, val-loss: 0.068 @ 2020-05-09 03:32:25.119277\n",
      "[3,  1320] loss: 0.120, val-loss: 0.027 @ 2020-05-09 03:32:57.436964\n",
      "[3,  1350] loss: 0.131, val-loss: 0.098 @ 2020-05-09 03:33:35.614019\n",
      "[3,  1380] loss: 0.133, val-loss: 0.061 @ 2020-05-09 03:34:25.670637\n",
      "[3,  1410] loss: 0.125, val-loss: 0.117 @ 2020-05-09 03:34:59.417963\n",
      "[3,  1440] loss: 0.104, val-loss: 0.077 @ 2020-05-09 03:35:40.799435\n",
      "[3,  1470] loss: 0.121, val-loss: 0.127 @ 2020-05-09 03:36:20.258169\n",
      "[3,  1500] loss: 0.141, val-loss: 0.019 @ 2020-05-09 03:36:59.373765\n",
      "[3,  1530] loss: 0.101, val-loss: 0.067 @ 2020-05-09 03:37:41.452983\n",
      "[3,  1560] loss: 0.132, val-loss: 0.067 @ 2020-05-09 03:38:20.068430\n",
      "[3,  1590] loss: 0.095, val-loss: 0.114 @ 2020-05-09 03:39:03.077150\n",
      "[3,  1620] loss: 0.094, val-loss: 0.033 @ 2020-05-09 03:39:43.338073\n",
      "[3,  1650] loss: 0.144, val-loss: 0.128 @ 2020-05-09 03:40:27.044438\n",
      "[3,  1680] loss: 0.086, val-loss: 0.120 @ 2020-05-09 03:41:05.269795\n",
      "[3,  1710] loss: 0.102, val-loss: 0.104 @ 2020-05-09 03:41:44.785778\n",
      "[3,  1740] loss: 0.109, val-loss: 0.055 @ 2020-05-09 03:42:25.927349\n",
      "[3,  1770] loss: 0.077, val-loss: 0.086 @ 2020-05-09 03:43:10.610574\n",
      "[3,  1800] loss: 0.117, val-loss: 0.211 @ 2020-05-09 03:43:54.095963\n",
      "[3,  1830] loss: 0.097, val-loss: 0.009 @ 2020-05-09 03:44:33.654342\n",
      "[3,  1860] loss: 0.143, val-loss: 0.061 @ 2020-05-09 03:45:10.910001\n",
      "[3,  1890] loss: 0.140, val-loss: 0.068 @ 2020-05-09 03:45:57.392979\n",
      "[3,  1920] loss: 0.099, val-loss: 0.190 @ 2020-05-09 03:46:34.812347\n",
      "[3,  1950] loss: 0.104, val-loss: 0.050 @ 2020-05-09 03:47:13.075515\n",
      "[3,  1980] loss: 0.097, val-loss: 0.013 @ 2020-05-09 03:47:59.632958\n",
      "[3,  2010] loss: 0.102, val-loss: 0.029 @ 2020-05-09 03:48:43.453899\n",
      "[3,  2040] loss: 0.132, val-loss: 0.007 @ 2020-05-09 03:49:32.575380\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b655cba5820e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# print every 30 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# @TODO average val loss over the 30 batches too?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_val_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%d, %5d] loss: %.3f, val-loss: %.3f @ %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d5c61017c37a>\u001b[0m in \u001b[0;36mget_val_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "#Train it\n",
    "%time\n",
    "losses = []\n",
    "val_losses = []\n",
    "for stn in stns:\n",
    "    stn.eval()\n",
    "\n",
    "print(\"Starting to train at: \", datetime.datetime.now(), \" (time is +7 w.r.t. our timezone)\")\n",
    "\n",
    "#f.write('Initial Validation F1: %.6f' % validate())\n",
    "\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    print('Starting epoch %d' % (epoch+1))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data[0].to(device), data[1]['genre'].to(device)  #data[1]['{argument for agf being trained}']\n",
    "        \n",
    "        input_mlp = None\n",
    "        with torch.no_grad():\n",
    "            out_intermediates = [stn.module.forward_intermediate(inputs, layers[i]) for i,stn in enumerate(stns)]\n",
    "            input_mlp = torch.cat(out_intermediates, dim=1)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = mlp(input_mlp)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 30 == 29:    # print every 30 mini-batches\n",
    "            # @TODO average val loss over the 30 batches too?\n",
    "            val_loss = get_val_loss()\n",
    "            avg_loss = running_loss / 30\n",
    "            print('[%d, %5d] loss: %.3f, val-loss: %.3f @ %s' % (epoch + 1,i + 1, avg_loss, val_loss, datetime.datetime.now()))\n",
    "            losses.append(avg_loss)\n",
    "            val_losses.append(float(val_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "final_f1 = validate()\n",
    "np.array(val_losses).tofile(f'logs/val_losses_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}')\n",
    "np.array(losses).tofile(f'logs/losses_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}')\n",
    "np.array(final_f1).tofile(f'logs/final_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'../models/MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}'\n",
    "torch.save(mlp, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Eval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'../models/MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}'\n",
    "mlp = torch.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load losses and final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.fromfile(f'logs/losses_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}')\n",
    "val_losses = np.fromfile(f'logs/val_losses_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}')\n",
    "final_f1 = np.fromfile(f'logs/final_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layers_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Performance History\n",
    "* SGM layer 4: 0.67209446\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
